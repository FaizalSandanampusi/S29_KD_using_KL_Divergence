{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Knowledge Distillation using KL Divergence Loss in PyTorch\n\nThis notebook demonstrates knowledge distillation (KD) using KL divergence loss on the CIFAR‑10 dataset in PyTorch. We first build and train a high‑capacity teacher CNN that achieves high accuracy on CIFAR‑10. Then, we define a simpler student model and train it in two ways:\n\n1. Using standard cross‑entropy loss (normal training).\n2. Using knowledge distillation (KD) that combines cross‑entropy loss with KL divergence loss between the teacher’s and student’s softened outputs.\n\nAt the end, we compare the test performance of the teacher, the normally trained student, and the KD-trained student, and provide a conclusion.","metadata":{"_uuid":"5cee0904-1b69-4be6-bbbd-d1af3458b65f","_cell_guid":"057da6be-ea94-4711-bbb7-3313ec00fe04","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1. Setup and Imports\n\nWe import PyTorch, torchvision, and other necessary libraries. We set seeds for reproducibility and define the device.","metadata":{"_uuid":"badf435b-7835-4d36-abe3-758f4f994617","_cell_guid":"7479caaa-4c9e-4e86-a28f-9138d8268b88","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport time\n\n# Set device (use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# For reproducibility\n# torch.manual_seed(42)\n# np.random.seed(42)","metadata":{"_uuid":"d43e48f5-9481-4826-852c-ce9b80d14331","_cell_guid":"4a8a0070-5b16-4ba4-8fb4-3dbf9c845a5a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T12:39:58.393699Z","iopub.execute_input":"2025-02-16T12:39:58.393984Z","iopub.status.idle":"2025-02-16T12:40:04.462593Z","shell.execute_reply.started":"2025-02-16T12:39:58.393952Z","shell.execute_reply":"2025-02-16T12:40:04.461476Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Load and Preprocess the CIFAR‑10 Dataset\n\nWe load the CIFAR‑10 dataset using torchvision. The images are converted to tensors and normalized. Data loaders are created for training and testing.","metadata":{"_uuid":"d8703d2e-ea8e-482e-b9d7-04125651eee3","_cell_guid":"d4300e86-9bb2-4617-b0fb-cea763fbfe57","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\ntransform =  transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n\nbatch_size = 256\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0,pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\nprint(\"Training set size:\", len(train_dataset))\nprint(\"Test set size:\", len(test_dataset))\n","metadata":{"_uuid":"84343ad4-3c45-49e7-9903-9fb0e6702a2c","_cell_guid":"59b5dedf-ddad-464b-ab23-70f01dd07ffa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T13:06:01.606052Z","iopub.execute_input":"2025-02-16T13:06:01.606372Z","iopub.status.idle":"2025-02-16T13:06:03.423003Z","shell.execute_reply.started":"2025-02-16T13:06:01.606347Z","shell.execute_reply":"2025-02-16T13:06:03.422031Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nTraining set size: 50000\nTest set size: 10000\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 3. Build and Train the Teacher Model\n\nThe teacher model is a deep CNN with three convolutional blocks and a fully connected classifier. It is trained using standard cross‑entropy loss.","metadata":{"_uuid":"bc7858cf-0bbe-47c0-aa23-e43479aa50f4","_cell_guid":"bc1d0c27-ca9d-4c94-a320-21c3c0bb156e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\n\nclass TeacherNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(TeacherNet, self).__init__()\n        # Block 1\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        # Block 2\n        self.block2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        # Block 3\n        self.block3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        # Classification block\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256 * 4 * 4, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.classifier(x)\n        return x\n\n# Initialize model and move to device\nteacher = TeacherNet().to(device)\nprint(teacher)","metadata":{"_uuid":"ff5e1f7c-eed9-4880-947c-c14c554a0318","_cell_guid":"9a9913b9-3032-44cf-b75c-2ddae88ad06c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T12:40:17.065137Z","iopub.execute_input":"2025-02-16T12:40:17.065479Z","iopub.status.idle":"2025-02-16T12:40:17.382945Z","shell.execute_reply.started":"2025-02-16T12:40:17.065453Z","shell.execute_reply":"2025-02-16T12:40:17.382048Z"}},"outputs":[{"name":"stdout","text":"TeacherNet(\n  (block1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=4096, out_features=512, bias=True)\n    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def train_teacher(model, train_loader, test_loader, num_epochs=50, lr=0.1):\n    # Use SGD with momentum and weight decay for better generalization\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Cosine Annealing LR Scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    \n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        start_time = time.time()\n        \n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        # Step the scheduler at the end of the epoch\n        scheduler.step()\n        \n        # Evaluate on test set\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        acc = 100. * correct / total\n        print(f\"Teacher Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}, Test Acc: {acc:.2f}%, Time: {time.time()-start_time:.1f}s\")\n        \n        if acc > best_acc:\n            best_acc = acc\n    print(f\"Best teacher accuracy: {best_acc:.2f}%\")\n\n# Uncomment to train teacher model\ntrain_teacher(teacher, train_loader, test_loader, num_epochs=50, lr=0.1)\n","metadata":{"_uuid":"96c0e5e7-2794-439f-9dca-e9258c0c8d86","_cell_guid":"2789815a-be09-411e-90b7-5cc7e01a838b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T12:40:46.176162Z","iopub.execute_input":"2025-02-16T12:40:46.176463Z","iopub.status.idle":"2025-02-16T12:52:08.391839Z","shell.execute_reply.started":"2025-02-16T12:40:46.176439Z","shell.execute_reply":"2025-02-16T12:52:08.390696Z"}},"outputs":[{"name":"stdout","text":"Teacher Epoch 1/50 - Loss: 0.6567, Test Acc: 77.80%, Time: 13.0s\nTeacher Epoch 2/50 - Loss: 0.5928, Test Acc: 78.66%, Time: 13.1s\nTeacher Epoch 3/50 - Loss: 0.5557, Test Acc: 79.82%, Time: 13.9s\nTeacher Epoch 4/50 - Loss: 0.5289, Test Acc: 80.47%, Time: 13.2s\nTeacher Epoch 5/50 - Loss: 0.5013, Test Acc: 81.20%, Time: 13.7s\nTeacher Epoch 6/50 - Loss: 0.4774, Test Acc: 81.63%, Time: 13.8s\nTeacher Epoch 7/50 - Loss: 0.4557, Test Acc: 82.13%, Time: 13.7s\nTeacher Epoch 8/50 - Loss: 0.4340, Test Acc: 82.82%, Time: 14.1s\nTeacher Epoch 9/50 - Loss: 0.4161, Test Acc: 82.93%, Time: 13.6s\nTeacher Epoch 10/50 - Loss: 0.3926, Test Acc: 82.97%, Time: 13.9s\nTeacher Epoch 11/50 - Loss: 0.3764, Test Acc: 83.18%, Time: 13.5s\nTeacher Epoch 12/50 - Loss: 0.3623, Test Acc: 83.61%, Time: 13.8s\nTeacher Epoch 13/50 - Loss: 0.3452, Test Acc: 83.26%, Time: 13.5s\nTeacher Epoch 14/50 - Loss: 0.3291, Test Acc: 84.36%, Time: 13.7s\nTeacher Epoch 15/50 - Loss: 0.3139, Test Acc: 84.44%, Time: 13.8s\nTeacher Epoch 16/50 - Loss: 0.3030, Test Acc: 84.95%, Time: 13.5s\nTeacher Epoch 17/50 - Loss: 0.2899, Test Acc: 84.73%, Time: 13.8s\nTeacher Epoch 18/50 - Loss: 0.2786, Test Acc: 85.67%, Time: 13.4s\nTeacher Epoch 19/50 - Loss: 0.2648, Test Acc: 85.19%, Time: 13.7s\nTeacher Epoch 20/50 - Loss: 0.2552, Test Acc: 85.47%, Time: 13.7s\nTeacher Epoch 21/50 - Loss: 0.2416, Test Acc: 85.64%, Time: 13.5s\nTeacher Epoch 22/50 - Loss: 0.2285, Test Acc: 85.74%, Time: 13.8s\nTeacher Epoch 23/50 - Loss: 0.2195, Test Acc: 85.55%, Time: 13.5s\nTeacher Epoch 24/50 - Loss: 0.2078, Test Acc: 86.87%, Time: 13.7s\nTeacher Epoch 25/50 - Loss: 0.1991, Test Acc: 86.30%, Time: 13.5s\nTeacher Epoch 26/50 - Loss: 0.1876, Test Acc: 86.32%, Time: 13.4s\nTeacher Epoch 27/50 - Loss: 0.1778, Test Acc: 87.51%, Time: 13.9s\nTeacher Epoch 28/50 - Loss: 0.1706, Test Acc: 86.97%, Time: 13.6s\nTeacher Epoch 29/50 - Loss: 0.1626, Test Acc: 86.40%, Time: 14.0s\nTeacher Epoch 30/50 - Loss: 0.1556, Test Acc: 88.13%, Time: 13.5s\nTeacher Epoch 31/50 - Loss: 0.1448, Test Acc: 87.10%, Time: 14.1s\nTeacher Epoch 32/50 - Loss: 0.1383, Test Acc: 87.58%, Time: 13.5s\nTeacher Epoch 33/50 - Loss: 0.1285, Test Acc: 87.89%, Time: 13.6s\nTeacher Epoch 34/50 - Loss: 0.1242, Test Acc: 88.20%, Time: 13.8s\nTeacher Epoch 35/50 - Loss: 0.1169, Test Acc: 88.28%, Time: 13.4s\nTeacher Epoch 36/50 - Loss: 0.1146, Test Acc: 88.25%, Time: 13.9s\nTeacher Epoch 37/50 - Loss: 0.1066, Test Acc: 87.98%, Time: 13.5s\nTeacher Epoch 38/50 - Loss: 0.1008, Test Acc: 88.18%, Time: 13.5s\nTeacher Epoch 39/50 - Loss: 0.0974, Test Acc: 88.61%, Time: 14.0s\nTeacher Epoch 40/50 - Loss: 0.0955, Test Acc: 88.74%, Time: 13.5s\nTeacher Epoch 41/50 - Loss: 0.0918, Test Acc: 89.04%, Time: 13.8s\nTeacher Epoch 42/50 - Loss: 0.0883, Test Acc: 88.63%, Time: 13.5s\nTeacher Epoch 43/50 - Loss: 0.0858, Test Acc: 88.73%, Time: 13.7s\nTeacher Epoch 44/50 - Loss: 0.0830, Test Acc: 88.77%, Time: 13.6s\nTeacher Epoch 45/50 - Loss: 0.0832, Test Acc: 88.50%, Time: 13.5s\nTeacher Epoch 46/50 - Loss: 0.0809, Test Acc: 88.66%, Time: 13.9s\nTeacher Epoch 47/50 - Loss: 0.0804, Test Acc: 88.42%, Time: 13.5s\nTeacher Epoch 48/50 - Loss: 0.0787, Test Acc: 89.04%, Time: 13.8s\nTeacher Epoch 49/50 - Loss: 0.0771, Test Acc: 88.98%, Time: 13.5s\nTeacher Epoch 50/50 - Loss: 0.0785, Test Acc: 88.58%, Time: 13.9s\nBest teacher accuracy: 89.04%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 4. Build and Train the Student Model (Without KD)\n\nThe student model is a simpler CNN. It is first trained using standard cross‑entropy loss.","metadata":{"_uuid":"a67eebb1-ebd5-4518-a2d8-b752c8c9125b","_cell_guid":"2dacd6d4-15f9-4214-8f1d-5eff7ce1c70d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class StudentNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(StudentNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nstudent_normal = StudentNet().to(device)\nprint(student_normal)","metadata":{"_uuid":"595342c1-af2a-4ea9-bdfe-dcc4626f2faa","_cell_guid":"031caa2e-3467-4060-92e0-ee9e529f11a7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T13:17:11.631655Z","iopub.execute_input":"2025-02-16T13:17:11.632001Z","iopub.status.idle":"2025-02-16T13:17:11.645584Z","shell.execute_reply.started":"2025-02-16T13:17:11.631973Z","shell.execute_reply":"2025-02-16T13:17:11.644708Z"}},"outputs":[{"name":"stdout","text":"StudentNet(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef train_student_normal(model, train_loader, test_loader, num_epochs=25, lr=0.005):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            targets_onehot = F.one_hot(targets, num_classes=10).float().to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets_onehot)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        acc = 100. * correct / total\n        print(f\"Student Normal Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}, Test Acc: {acc:.2f}%\")\n  \n# Uncomment to train student model using MSELoss\ntrain_student_normal(student_normal, train_loader, test_loader, num_epochs=25, lr=0.0001)\n","metadata":{"_uuid":"deefcf57-c8c0-4645-a772-56b1a3b9c94a","_cell_guid":"3b7f10aa-50c4-49b6-94d5-b8799ee913bb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T13:17:12.291146Z","iopub.execute_input":"2025-02-16T13:17:12.291578Z","iopub.status.idle":"2025-02-16T13:28:19.274403Z","shell.execute_reply.started":"2025-02-16T13:17:12.291542Z","shell.execute_reply":"2025-02-16T13:28:19.273539Z"}},"outputs":[{"name":"stdout","text":"Student Normal Epoch 1/25 - Loss: 0.0804, Test Acc: 41.81%\nStudent Normal Epoch 2/25 - Loss: 0.0736, Test Acc: 46.45%\nStudent Normal Epoch 3/25 - Loss: 0.0708, Test Acc: 49.49%\nStudent Normal Epoch 4/25 - Loss: 0.0686, Test Acc: 51.46%\nStudent Normal Epoch 5/25 - Loss: 0.0667, Test Acc: 53.92%\nStudent Normal Epoch 6/25 - Loss: 0.0651, Test Acc: 54.65%\nStudent Normal Epoch 7/25 - Loss: 0.0636, Test Acc: 56.18%\nStudent Normal Epoch 8/25 - Loss: 0.0624, Test Acc: 56.55%\nStudent Normal Epoch 9/25 - Loss: 0.0612, Test Acc: 57.45%\nStudent Normal Epoch 10/25 - Loss: 0.0602, Test Acc: 58.88%\nStudent Normal Epoch 11/25 - Loss: 0.0593, Test Acc: 58.45%\nStudent Normal Epoch 12/25 - Loss: 0.0585, Test Acc: 59.66%\nStudent Normal Epoch 13/25 - Loss: 0.0576, Test Acc: 61.10%\nStudent Normal Epoch 14/25 - Loss: 0.0569, Test Acc: 60.64%\nStudent Normal Epoch 15/25 - Loss: 0.0562, Test Acc: 60.80%\nStudent Normal Epoch 16/25 - Loss: 0.0556, Test Acc: 62.56%\nStudent Normal Epoch 17/25 - Loss: 0.0548, Test Acc: 63.02%\nStudent Normal Epoch 18/25 - Loss: 0.0544, Test Acc: 62.64%\nStudent Normal Epoch 19/25 - Loss: 0.0538, Test Acc: 63.01%\nStudent Normal Epoch 20/25 - Loss: 0.0534, Test Acc: 64.28%\nStudent Normal Epoch 21/25 - Loss: 0.0529, Test Acc: 63.81%\nStudent Normal Epoch 22/25 - Loss: 0.0525, Test Acc: 64.64%\nStudent Normal Epoch 23/25 - Loss: 0.0520, Test Acc: 64.95%\nStudent Normal Epoch 24/25 - Loss: 0.0517, Test Acc: 65.60%\nStudent Normal Epoch 25/25 - Loss: 0.0512, Test Acc: 65.61%\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## 5. Train the Student Model with Knowledge Distillation (KD)\n\nThe student model is re‑trained using knowledge distillation. In each training step:\n- The teacher (in eval mode) produces softened predictions using a temperature parameter.\n- The student produces predictions.\n- We compute the hard loss using cross‑entropy with true labels.\n- We compute the KD loss using KL divergence between the student’s and teacher’s softened predictions.\n- The total loss is a weighted sum:\n\n\ntotal_loss = \\$ \\alpha \\$ * CE_loss + (1 - \\$ \\alpha \\$) * (temperature**2) * KD_loss\n\n\nKLDivLoss in PyTorch expects the input as log‑probabilities and the target as probabilities.","metadata":{"_uuid":"ac705568-439e-4a3d-a1b1-d646966baece","_cell_guid":"92b09250-db8b-4463-8ba2-e8c9475fc80b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nstudent_kd = StudentNet().to(device)\n\ndef train_student_kd(student, teacher, train_loader, test_loader, num_epochs=25, lr=0.001, temperature=5.0, alpha=0.5):\n    optimizer = optim.Adam(student.parameters(), lr=lr)\n    ce_loss_fn = nn.CrossEntropyLoss()\n    kd_loss_fn = nn.KLDivLoss(reduction='batchmean')\n    \n    teacher.eval()  # Freeze teacher\n    \n    for epoch in range(num_epochs):\n        student.train()\n        running_loss = 0.0\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            \n            with torch.no_grad():\n                teacher_outputs = teacher(inputs)\n                teacher_soft = F.softmax(teacher_outputs / temperature, dim=1)\n            \n            student_outputs = student(inputs)\n            ce_loss = ce_loss_fn(student_outputs, targets)\n            student_log_soft = F.log_softmax(student_outputs / temperature, dim=1)\n            kd_loss = kd_loss_fn(student_log_soft, teacher_soft)\n            loss = alpha * ce_loss + (1 - alpha) * (temperature**2) * kd_loss\n            \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            \n        student.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = student(inputs)\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        acc = 100. * correct / total\n        print(f\"Student KD Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}, Test Acc: {acc:.2f}%\")\n        \n# Uncomment to train student model with KD\ntrain_student_kd(student_kd, teacher, train_loader, test_loader, num_epochs=30, lr=0.001, temperature=4.5, alpha=0.8)\n","metadata":{"_uuid":"71e0ebec-70a5-42ab-9441-0a7f2bcf3c47","_cell_guid":"c8d1999e-b73e-4843-8eac-317a8d146abf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T12:57:06.516251Z","iopub.execute_input":"2025-02-16T12:57:06.516620Z","iopub.status.idle":"2025-02-16T13:03:08.543934Z","shell.execute_reply.started":"2025-02-16T12:57:06.516590Z","shell.execute_reply":"2025-02-16T13:03:08.542664Z"}},"outputs":[{"name":"stdout","text":"Student KD Epoch 1/30 - Loss: 3.4799, Test Acc: 51.18%\nStudent KD Epoch 2/30 - Loss: 2.7296, Test Acc: 55.03%\nStudent KD Epoch 3/30 - Loss: 2.4162, Test Acc: 60.62%\nStudent KD Epoch 4/30 - Loss: 2.2296, Test Acc: 63.10%\nStudent KD Epoch 5/30 - Loss: 2.0980, Test Acc: 62.96%\nStudent KD Epoch 6/30 - Loss: 1.9840, Test Acc: 65.68%\nStudent KD Epoch 7/30 - Loss: 1.8619, Test Acc: 66.69%\nStudent KD Epoch 8/30 - Loss: 1.7834, Test Acc: 68.14%\nStudent KD Epoch 9/30 - Loss: 1.7143, Test Acc: 69.56%\nStudent KD Epoch 10/30 - Loss: 1.6416, Test Acc: 70.69%\nStudent KD Epoch 11/30 - Loss: 1.5712, Test Acc: 70.61%\nStudent KD Epoch 12/30 - Loss: 1.5153, Test Acc: 71.22%\nStudent KD Epoch 13/30 - Loss: 1.4909, Test Acc: 71.46%\nStudent KD Epoch 14/30 - Loss: 1.4410, Test Acc: 72.72%\nStudent KD Epoch 15/30 - Loss: 1.4134, Test Acc: 72.68%\nStudent KD Epoch 16/30 - Loss: 1.3639, Test Acc: 73.95%\nStudent KD Epoch 17/30 - Loss: 1.3435, Test Acc: 73.83%\nStudent KD Epoch 18/30 - Loss: 1.3235, Test Acc: 74.48%\nStudent KD Epoch 19/30 - Loss: 1.2857, Test Acc: 74.40%\nStudent KD Epoch 20/30 - Loss: 1.2654, Test Acc: 73.98%\nStudent KD Epoch 21/30 - Loss: 1.2487, Test Acc: 74.71%\nStudent KD Epoch 22/30 - Loss: 1.2288, Test Acc: 75.05%\nStudent KD Epoch 23/30 - Loss: 1.2074, Test Acc: 75.17%\nStudent KD Epoch 24/30 - Loss: 1.1815, Test Acc: 75.75%\nStudent KD Epoch 25/30 - Loss: 1.1651, Test Acc: 75.57%\nStudent KD Epoch 26/30 - Loss: 1.1514, Test Acc: 75.50%\nStudent KD Epoch 27/30 - Loss: 1.1479, Test Acc: 76.80%\nStudent KD Epoch 28/30 - Loss: 1.1283, Test Acc: 75.78%\nStudent KD Epoch 29/30 - Loss: 1.1078, Test Acc: 77.10%\nStudent KD Epoch 30/30 - Loss: 1.0885, Test Acc: 76.40%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 6. Comparison and Conclusion\n\nThe following cell compares the test accuracies of:\n- The Teacher Model\n- The Student Model trained without KD (normal training)\n- The Student Model trained with KD\n\nBased on the results, we conclude how knowledge distillation (using KL divergence loss) can help the student model achieve performance closer to the teacher model.","metadata":{"_uuid":"1c1a5e68-c97e-4292-824d-a5ccc1fc62bf","_cell_guid":"2d474630-e49b-4712-b10b-d05fd9bb4505","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def evaluate(model, data_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    return 100. * correct / total\n\nteacher_acc = evaluate(teacher, test_loader)\nstudent_normal_acc = evaluate(student_normal, test_loader)\nstudent_kd_acc = evaluate(student_kd, test_loader)\n\nprint(f\"Teacher Model Test Accuracy: {teacher_acc:.2f}%\")\nprint(f\"Student Model (Normal Training) Test Accuracy: {student_normal_acc:.2f}%\")\nprint(f\"Student Model (KD Training) Test Accuracy: {student_kd_acc:.2f}%\")","metadata":{"_uuid":"0992b72b-6b84-4820-a068-f9204eb61778","_cell_guid":"cec1935c-b4dc-468d-a674-3f1fa988fc8b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-16T13:29:31.905536Z","iopub.execute_input":"2025-02-16T13:29:31.905893Z","iopub.status.idle":"2025-02-16T13:29:45.061742Z","shell.execute_reply.started":"2025-02-16T13:29:31.905865Z","shell.execute_reply":"2025-02-16T13:29:45.060777Z"}},"outputs":[{"name":"stdout","text":"Teacher Model Test Accuracy: 88.77%\nStudent Model (Normal Training) Test Accuracy: 65.59%\nStudent Model (KD Training) Test Accuracy: 76.84%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook:\n- A high‑capacity teacher model was trained on CIFAR‑10 using standard cross‑entropy loss.\n- A simpler student model was trained in two ways:\n  - Using normal training with MSE loss.\n  - Using knowledge distillation, where the student was trained with a combination of cross‑entropy loss and KL divergence loss (comparing softened predictions of the teacher and student).\n- The comparison shows that knowledge distillation can help the student model achieve improved performance, narrowing the gap between the student and the teacher.\n\nAdjusting hyperparameters (such as temperature and the loss weight alpha) and training duration can further enhance performance.","metadata":{"_uuid":"3d3f5193-862b-485b-ad8d-000db63028e5","_cell_guid":"61708cd9-3cd5-409f-9e79-8813ece5e4a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}